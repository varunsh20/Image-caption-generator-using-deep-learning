# Image-caption-generator-using-deep-learning

The goal of this project is to create a deep learning model which takes an image as an
input and generates the most suitable caption related to that image as output.It is trained on
Flicker/8k dataset available on kaggle and it uses machine learning and deep learning
techniques like CNN’S, RNN’S , LSTM’S, Feed forward neural network,Word Embeddings,
language modelling,and transfer learning.
Here is the link to dataset:
https://www.kaggle.com/ming666/flicker8k-dataset/notebooks

Here all the training images contains five captions related to that image.So firstly,we created a dictionary having keys as the images and values as all captions related to that image.Then we create a vocabulary which consists of only frequently occuring unique words i.e words that have occured at least 10 times all over the training data.This helps us to reduce the size of the vocabulary.
With the help of transfer learning through Resnet50 model we extract the feature vectors of all the images(training and test).
Then all the words in the vocab are assigned to a unique number so as to convert them into numerical values.
The problem is converted into a supervised learning problem in which we create a data generator object in which X contains an array of image features and partial captions and Y has the next word in the sequence.Then on the basis of these features the model predicts the new word this is called language modelling.
Before feeding the word indexes into RNN model they are passed through an embedding layer which converts a word into a 50 dim-vector.Here we used glove embedding.
Now three models are created one which converts a 2048 dimensional image vector into 256 dimensional vector,one for converting sentences into 256 dimensional vector and then the output of these two models are combined in another model whose final output gives the probability of every word in the vocab after which we can take the argmax so that we get the index of word having maximum probability. The process takes place as follows.

![ee1](https://user-images.githubusercontent.com/62187533/121786680-2db64e80-cbdf-11eb-8f82-fc496e3a1377.jpeg)


## Here are some of the captions generated by our model:

![Screenshot (148)](https://user-images.githubusercontent.com/62187533/121786919-89350c00-cbe0-11eb-959c-c5132426ba18.png)
![Screenshot (149)](https://user-images.githubusercontent.com/62187533/121786920-89350c00-cbe0-11eb-986c-1afdd6b01060.png)
![Screenshot (150)](https://user-images.githubusercontent.com/62187533/121786922-89cda280-cbe0-11eb-86a8-5b45fb7286bc.png)
![Screenshot (151)](https://user-images.githubusercontent.com/62187533/121786923-8a663900-cbe0-11eb-9b29-a0434e965896.png)
![Screenshot (152)](https://user-images.githubusercontent.com/62187533/121786926-8b976600-cbe0-11eb-9377-613151a6aca6.png)
![Screenshot (153)](https://user-images.githubusercontent.com/62187533/121786929-8c2ffc80-cbe0-11eb-9598-67a0f95e8638.png)
![Screenshot (154)](https://user-images.githubusercontent.com/62187533/121786931-8cc89300-cbe0-11eb-8c28-cfb3630f6ddb.png)

## Here is how the web app of this project looks like
![Screenshot (114)](https://user-images.githubusercontent.com/62187533/121787071-a6b6a580-cbe1-11eb-9f09-3a0fa05f9432.png)
![Screenshot (115)](https://user-images.githubusercontent.com/62187533/121787047-7bcc5180-cbe1-11eb-9c00-d2760fbf6975.png)
![Screenshot (116)](https://user-images.githubusercontent.com/62187533/121787049-7bcc5180-cbe1-11eb-9cf5-0b28ddb5eb70.png)

## References:
https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8
